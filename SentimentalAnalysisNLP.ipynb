{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnTmaW13PaiXn1iUp+/tXX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ragulan-K/ML-Projects-24/blob/main/SentimentalAnalysisNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRS74HNAh4fq"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries"
      ],
      "metadata": {
        "id": "jbqVICmSid5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re #Regular expressions\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "R9GOty64ipVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Dataset from Local Directory"
      ],
      "metadata": {
        "id": "8PaZ5nLXirzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "YISQsdZPivXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Dataset"
      ],
      "metadata": {
        "id": "Sz4pggfwixev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('dataset.csv')\n",
        "print(dataset.shape)\n",
        "print(dataset.head(5))"
      ],
      "metadata": {
        "id": "pmV6w53Mi16I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Segregating Dataset into Input & Output"
      ],
      "metadata": {
        "id": "q8T5-5eyi5nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = dataset.iloc[:, 10].values\n",
        "labels = dataset.iloc[:, 1].values\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "cUB8xKV1i9Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Removing the Special Character"
      ],
      "metadata": {
        "id": "PYZ_nOvfi_eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_features = []\n",
        "\n",
        "for sentence in range(0, len(features)):\n",
        "    # Remove all the special characters\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "metadata": {
        "id": "6lNi7vywjFxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Extraction from text"
      ],
      "metadata": {
        "id": "df3U-y8WjIWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
        "processed_features = vectorizer.fit_transform(processed_features).toarray()\n",
        "print(processed_features)"
      ],
      "metadata": {
        "id": "UrjH83ZSjPbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting Dataset into Train & Test"
      ],
      "metadata": {
        "id": "1lwsie8jjRhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "vIghT6cDjVMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Random Forest Algorithm"
      ],
      "metadata": {
        "id": "ckA5t0xjjXvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RddB2OoejbbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicting the Test data with Trained Model"
      ],
      "metadata": {
        "id": "CtNBz_J1jdZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "F7q74YVcjgy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Score of the Model"
      ],
      "metadata": {
        "id": "dVaNZOkNji-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "id": "Mo9izebWjq0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Confusion Matrix"
      ],
      "metadata": {
        "id": "P_QDbFInjuB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, predictions, labels=['negative', 'neutral', 'positive'])\n",
        "plot_confusion_matrix(cm, classes=['negative', 'neutral', 'positive'])"
      ],
      "metadata": {
        "id": "5L2k00QGjyXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}